days_since_nearest_heb_cel <- foreach(i = 1:nrow(data), .combine = c, .packages = "base") %dopar% {
# Extract coordinates for the i-th row of 'data'
xii <- xi[i]
distance_from_i <- abs(xii - xj)
# Return the index of the minimum distance
c(which.min(distance_from_i), min(distance_from_i))
}
library(foreach)
library(doParallel)
# Set up the parallel backend
num_cores <- round(parallel::detectCores()/2)
cl <- makeCluster(num_cores)
registerDoParallel(cl)
days_since_nearest_heb_cel <- foreach(i = 1:nrow(data), .combine = c, .packages = "base") %dopar% {
# Extract coordinates for the i-th row of 'data'
xii <- xi[i]
distance_from_i <- abs(xii - xj)
# Return the index of the minimum distance
c(which.min(distance_from_i), min(distance_from_i))
}
# Stop the parallel backend
stopCluster(cl)
days_since_nearest_heb_cel
num_cores <- round(parallel::detectCores()/2)
cl <- makeCluster(num_cores)
registerDoParallel(cl)
days_since_nearest_heb_cel <- foreach(i = 1:nrow(data), .combine = rbind, .packages = "base") %dopar% {
# Extract coordinates for the i-th row of 'data'
xii <- xi[i]
distance_from_i <- abs(xii - xj)
# Return the index of the minimum distance
c(which.min(distance_from_i), min(distance_from_i))
}
# Stop the parallel backend
stopCluster(cl)
days_since_nearest_heb_cel
library(foreach)
library(doParallel)
# Set up the parallel backend
num_cores <- round(parallel::detectCores()/2)
cl <- makeCluster(num_cores)
registerDoParallel(cl)
days_since_nearest_heb_cel <- foreach(i = 1:nrow(data), .combine = rbind, .packages = "base") %dopar% {
# Extract coordinates for the i-th row of 'data'
xii <- xi[i]
distance_from_i <- abs(xii - xj)
# Return the index of the minimum distance
res <- c(which.min(distance_from_i), min(distance_from_i))
c(res, res[2]*sign(xii - xj[res[1]]))
}
# Stop the parallel backend
stopCluster(cl)
days_since_nearest_heb_cel
rownames(days_since_nearest_heb_cel)
rownames(days_since_nearest_heb_cel) <- NULL
head(days_since_nearest_heb_cel)
colnames(days_since_nearest_heb_cel) <- c("hebcel_id", "days_from_nearest_heb_cel", "days_from_nearest_heb_cel_signed")
rm(list = ls())
library(tidyverse)
first_year <- 1989
last_year <- 2023
years <- first_year:last_year
hebcal_list <- vector("list", length = length(years))
for(i in 1:length(years)){
file_dir <- paste0("hebcal/hebcal_", years[i], "_eur.csv" )
file <- read.csv(file_dir)
ind <- grep(x = file$Start.Date, pattern = years[i])
hebcal_list[[i]] <- file[ind, ]
}
hebcal <- do.call(rbind, hebcal_list) |> select(Subject, Start.Date)
ymd(hebcal$Start.Date)
make_datetime(hebcal$Start.Date)
date_list <- lapply(strsplit(hebcal$Start.Date, "/"), as.integer)
date_list <- lapply(date_list, function(x){
as.character(make_datetime(year = x[3], month = x[2], day = x[1]))
})
rm(list = ls())
library(tidyverse)
first_year <- 1989
last_year <- 2023
years <- first_year:last_year
hebcal_list <- vector("list", length = length(years))
for(i in 1:length(years)){
file_dir <- paste0("hebcal/hebcal_", years[i], "_eur.csv" )
file <- read.csv(file_dir)
ind <- grep(x = file$Start.Date, pattern = years[i])
hebcal_list[[i]] <- file[ind, ]
}
hebcal <- do.call(rbind, hebcal_list) |> select(Subject, Start.Date)
ymd(hebcal$Start.Date)
make_datetime(hebcal$Start.Date)
date_list <- lapply(strsplit(hebcal$Start.Date, "/"), as.integer)
date_list <- lapply(date_list, function(x){
as.character(make_datetime(year = x[3], month = x[2], day = x[1]))
})
hebcal$Start.Date <- unlist(date_list)
hebcal$hebcal_id <- 1:nrow(hebcal)
write.csv(hebcal, "hebcal.csv", row.names = F)
## adding variable
# days_since_nearest_heb_cel
hebcal <- read.csv("hebcal.csv")
data <- read.csv("IS_PAL.csv")
xi <- as.Date(data$date_start, format = "%Y-%m-%d")
xj <- as.Date(hebcal$Start.Date, format = "%Y-%m-%d")
library(foreach)
library(doParallel)
# Set up the parallel backend
num_cores <- round(parallel::detectCores()/2)
cl <- makeCluster(num_cores)
registerDoParallel(cl)
days_since_nearest_heb_cal <- foreach(i = 1:nrow(data), .combine = rbind, .packages = "base") %dopar% {
# Extract coordinates for the i-th row of 'data'
xii <- xi[i]
distance_from_i <- abs(xii - xj)
# Return the index of the minimum distance
res <- c(which.min(distance_from_i), min(distance_from_i))
c(res, res[2]*sign(xii - xj[res[1]]))
}
# Stop the parallel backend
stopCluster(cl)
rownames(days_since_nearest_heb_cal) <- NULL
colnames(days_since_nearest_heb_cal) <- c("hebcal_id", "days_from_nearest_heb_cal", "days_from_nearest_heb_cal_signed")
head(days_since_nearest_heb_cal)
nrow(days_since_nearest_heb_cal)
data
colnames(data)
cbind(data, days_since_nearest_heb_cal)
str(cbind(data, days_since_nearest_heb_cal))
cbind(data, days_since_nearest_heb_cal)
data <- cbind(data, days_since_nearest_heb_cal)
write.csv("IS_PAL.csv", row.names = FALSE)
rm(list = ls())
# caricamento librerie
if(!require(tidyverse)) install.packages("tidyverse")
ged241 <- readRDS("ged241.rds")
data <- ged241 |>
filter(conflict_name == "Israel: Palestine" & country == "Israel") |>
select(id, year, side_b, adm_1, latitude, longitude, event_clarity, date_prec,
date_start, date_end, deaths_a, deaths_b, deaths_civilians,
deaths_unknown, best)
data <- data[order(data$date_start), ]
data$event_duration <- as.numeric((data$date_end - data$date_start)/(3600*24))
date_start <- data$date_start
date_start <- as.Date(date_start, format = "%Y-%m-%d")
days_since_start <- as.numeric(date_start - min(date_start))
data$days_since_start <- days_since_start
plot(data$days_since_start)
data$intertimes <- c(NA, diff(data$days_since_start))
head(data)
write.csv(data, "IS_PAL.csv")
rm(list = ls())
library(osmdata)
library(sf)
library(tidyverse)
# Definisci l'area geografica (esempio: Israele)
area2 <- "Israel"
# Query per luoghi di culto
places_of_worship_i <- opq(bbox = area2) |>
add_osm_feature(key = "amenity", value = "place_of_worship") |>
osmdata_sf()
# Estrai i dati dei punti (nodi)
points2 <- places_of_worship_i$osm_points |>
select(name, geometry) |>
mutate(Latitude = st_coordinates(geometry)[,2],
Longitude = st_coordinates(geometry)[,1]) |>
st_drop_geometry() # Rimuove la colonna geometrica, se non necessaria
# Rimuovi i punti senza nome
filtered_places2 <- points2 |>
filter(!is.na(name)) |>
select(Name = name, Latitude, Longitude)
# Salvataggio dati in un csv
rownames(filtered_places2) <- NULL
filtered_places2$id <- 1:nrow(filtered_places2)
write.csv(filtered_places2, "places_of_worship_israel.csv", row.names = FALSE)
#### filtraggio e merge con il dataset
w_isr <- read.csv("places_of_worship_israel.csv")
data <- read.csv("IS_PAL.csv")
xy_j <- as.matrix(w_isr[, c("Latitude", "Longitude")])
library(foreach)
library(doParallel)
# Set up the parallel backend
num_cores <- round(parallel::detectCores()/2)
cl <- makeCluster(num_cores)
registerDoParallel(cl)
# Parallel computation with foreach
min_distance_from_i <- foreach(i = 1:nrow(data), .combine = rbind, .packages = "base") %dopar% {
# Extract coordinates for the i-th row of 'data'
xy_i <- as.numeric(data[i, c("latitude", "longitude")])
distance_from_i <- apply(sweep(xy_j, 2, xy_i, "-"), 1, function(x) sum(sqrt(x^2)))
# Return the index of the minimum distance
c(which.min(distance_from_i), min(distance_from_i))
}
# Stop the parallel backend
stopCluster(cl)
nrow(min_distance_from_i)
rownames(min_distance_from_i) <- NULL
min_distance_from_i <- as.data.frame(min_distance_from_i)
head(min_distance_from_i)
colnames(min_distance_from_i) <- c("nearest_pow", "dist_nearest_pow")
write.csv(min_distance_from_i, "nearest_pow.csv")
data <- data.frame(data, min_distance_from_i)[,-1]
write.csv(data, "IS_PAL.csv", row.names = F)
rm(list = ls())
library(tidyverse)
first_year <- 1989
last_year <- 2023
years <- first_year:last_year
hebcal_list <- vector("list", length = length(years))
for(i in 1:length(years)){
file_dir <- paste0("hebcal/hebcal_", years[i], "_eur.csv" )
file <- read.csv(file_dir)
ind <- grep(x = file$Start.Date, pattern = years[i])
hebcal_list[[i]] <- file[ind, ]
}
hebcal <- do.call(rbind, hebcal_list) |> select(Subject, Start.Date)
date_list <- lapply(strsplit(hebcal$Start.Date, "/"), as.integer)
date_list <- lapply(date_list, function(x){
as.character(make_datetime(year = x[3], month = x[2], day = x[1]))
})
hebcal$Start.Date <- unlist(date_list)
hebcal$hebcal_id <- 1:nrow(hebcal)
write.csv(hebcal, "hebcal.csv", row.names = F)
## adding variable
# days_since_nearest_heb_cel
hebcal <- read.csv("hebcal.csv")
data <- read.csv("IS_PAL.csv")
xi <- as.Date(data$date_start, format = "%Y-%m-%d")
xj <- as.Date(hebcal$Start.Date, format = "%Y-%m-%d")
library(foreach)
library(doParallel)
# Set up the parallel backend
num_cores <- round(parallel::detectCores()/2)
cl <- makeCluster(num_cores)
registerDoParallel(cl)
days_since_nearest_heb_cal <- foreach(i = 1:nrow(data), .combine = rbind, .packages = "base") %dopar% {
# Extract coordinates for the i-th row of 'data'
xii <- xi[i]
distance_from_i <- abs(xii - xj)
# Return the index of the minimum distance
res <- c(which.min(distance_from_i), min(distance_from_i))
c(res, res[2]*sign(xii - xj[res[1]]))
}
# Stop the parallel backend
stopCluster(cl)
rownames(days_since_nearest_heb_cal) <- NULL
colnames(days_since_nearest_heb_cal) <- c("hebcal_id", "days_from_nearest_heb_cal", "days_from_nearest_heb_cal_signed")
data <- cbind(data, days_since_nearest_heb_cal)
write.csv("IS_PAL.csv", row.names = FALSE)
?write.csv()
write.csv(data, "IS_PAL.csv", row.names = FALSE)
# caricamento librerie
if(!require(tidyverse)) install.packages("tidyverse")
# if(!require(maps)) install.packages("maps")
# if(!require(mapproj)) install.packages("mapproj")
# if(!require(colorspace)) install.packages("colorspace")
# if(!require(latex2exp)) install.packages("latex2exp")
# if(!require(tmaptools)) install.packages("tmaptools")
# if(!require(sf)) install.packages("sf")
if(!require(stopp)) install.packages("stopp")
if(!require(spatstat)) install.packages("spatstat")
# if(!require(rnaturalearth)) install.packages("rnaturalearth")
# if(!require(rnaturalearthdata)) install.packages("rnaturalearthdata")
# Per il nostro lavoro, le variabili di interesse principali
# sono le coordinate geografiche dell'evento, la data di inizio
# dell'episodio di violenza e la stima più precisa del numero
# totale di morti.
# caricamento librerie
if(!require(tidyverse)) install.packages("tidyverse")
# if(!require(maps)) install.packages("maps")
# if(!require(mapproj)) install.packages("mapproj")
# if(!require(colorspace)) install.packages("colorspace")
# if(!require(latex2exp)) install.packages("latex2exp")
# if(!require(tmaptools)) install.packages("tmaptools")
# if(!require(sf)) install.packages("sf")
if(!require(stopp)) install.packages("stopp")
if(!require(spatstat)) install.packages("spatstat")
# if(!require(rnaturalearth)) install.packages("rnaturalearth")
# if(!require(rnaturalearthdata)) install.packages("rnaturalearthdata")
war <- read.csv("IS_PAL.csv")
war$side_b <- factor(war$side_b)
war$adm_1 <- factor(war$adm_1)
war$event_clarity <- factor(war$event_clarity)
war$date_prec <- factor(war$date_prec)
rmarkdown::paged_table(head(war, 10))
hist(war$event_duration, main = "Distribuzione della durata degli eventi", xlab = "durata degli eventi")
hist(war$event_duration[war$event_duration!=0], main = "Distribuzione della durata degli eventi
(tolti quelli con durata 0)", xlab = "durata degli eventi")
addmargins(table(war$event_clarity[war$event_duration!=0],
war$event_duration[war$event_duration!=0]))
#89 eventi su 4181 non si concludono nello stesso giorno, 52 degli 89 eventi si
#sono conclusi il giorno successivo a quello di inizio
#
#morti per anno
fatalities <- war |>
group_by(year) |>
summarise(fatalities_per_year = sum(best))
ggplot(fatalities, aes(x = year, y = fatalities_per_year)) +
geom_line(color = "black", linewidth = 1) +  # Linea blu
geom_point(color = "red", size = 2) + # Punti rossi per enfatizzare i dati
labs(title = "Serie Storica dei morti per Anno",
x = "Anno",
y = "Fatalità") +
theme_minimal()
#densità di event_duration condizionata a event_clarity
filtered_war <- war |>
filter(event_duration!=0)
#distribuzione di event_duration condizionata a event_clarity
ggplot(filtered_war, aes(x = event_duration, fill = event_clarity)) +
geom_bar(alpha = 0.7) +
facet_wrap(~ event_clarity)
intertimes_table <- table(war$intertimes[-1])
intertimes <- as.integer(names(intertimes_table))
n_total <- sum(intertimes_table)
abs_freq <- as.vector(intertimes_table)
rel_freq <- abs_freq/n_total
cum_freq <- cumsum(rel_freq)
surv <- (1 - cum_freq) + rel_freq
intertimes_distr <- data.frame(intertimes = intertimes, abs_freq = abs_freq,
rel_freq = rel_freq, cum_freq = cum_freq, surv = surv)
ggplot(intertimes_distr, aes(x = intertimes, y = rel_freq)) +
geom_col() + ylab("density") +
xlab("intertimes")
ggplot(intertimes_distr, aes(x = intertimes, y = cum_freq)) +
geom_point(shape = 1) +
geom_line(linewidth = 0.2) + ylab("ecdf") +
xlab("intertimes")
ggplot(intertimes_distr, aes(x = log(intertimes), y = log(surv))) +
geom_point(shape = 1) +
geom_line(linewidth = 0.2) +
ylab("log(survival function)") +
xlab("log(intertimes)") +
geom_smooth(method = "lm", se = FALSE, linewidth = 0.3, color = "red")
plot(ecdf(war$intertimes))
?qqplot()
x <- war$intertimes[-1]
qqplot(x)
qqplot(x, distribution = function(p) qexp(p, rate = 1/mean(x)))
x <- war$intertimes[-1]
n <- length(x)
lambda <- 1/mean(x)
theoretical_quantiles <- qexp(ppoints(n), rate = lambda)
# Sort the observed data
sorted_data <- sort(x)
# Create the QQ plot
qqplot(theoretical_quantiles, sorted_data,
main = "QQ Plot: Exponential Fit",
xlab = "Theoretical Quantiles",
ylab = "Sample Quantiles")
abline(0, 1, col = "red")  # Add a reference line
rexp(100, 1/mean(x))
hist(x)
x
statMod::mle(x, model = statMod::negbin1)
statMod::mle(x, model = statMod::negbin2)
statMod::mle(x, model = statMod::negbin1)
?MASS::fitdistr()
?MASS::fitdistr(x, densfun = "negative binomial")
MASS::fitdistr(x, densfun = "negative binomial")
x <- war$intertimes[-1]
exp_fit <- MASS::fitdistr(x, "exponential")
teor_distr <- pexp(intertimes_distr$intertimes, rate = exp_fit$estimate)
intertimes_distr$exp <- teor_distr
ggplot(data = intertimes_distr) +
geom_point(aes(x = exp, y = cum_freq), pch = 1) +
geom_abline(intercept = 0, slope = 1)
x <- war$intertimes[-1]
exp_fit <- MASS::fitdistr(x, "exponential")
teor_distr <- pexp(intertimes_distr$intertimes, rate = exp_fit$estimate)
intertimes_distr$exp <- teor_distr
ggplot(data = intertimes_distr) +
geom_point(aes(x = exp, y = cum_freq), pch = 1) +
geom_abline(intercept = 0, slope = 1)
x <- war$intertimes[-1]
n <- length(x)
lambda <- 1/mean(x)
theoretical_quantiles <- qexp(ppoints(n), rate = lambda)
# Sort the observed data
sorted_data <- sort(x)
# Create the QQ plot
qqplot(theoretical_quantiles, sorted_data,
main = "QQ Plot: Exponential Fit",
xlab = "Theoretical Quantiles",
ylab = "Sample Quantiles")
abline(0, 1, col = "red")  # Add a reference line
fit <- fitdistr(x, "Negative Binomial")
library(MASS)
fit <- fitdistr(x, "Negative Binomial")
# Extract the estimated parameters
size <- fit$estimate["size"]  # Dispersion parameter
mu <- fit$estimate["mu"]      # Mean parameter
size
mu
# Generate theoretical quantiles
n <- length(x)
theoretical_quantiles <- qnbinom(ppoints(n), size = size, mu = mu)
?ppoints()
# Sort the observed data
sorted_data <- sort(x)
# Create the QQ plot
qqplot(theoretical_quantiles, sorted_data,
main = "QQ Plot: Negative Binomial Fit",
xlab = "Theoretical Quantiles",
ylab = "Sample Quantiles")
abline(0, 1, col = "red")  # Add a reference line
?fitdistr
library(MASS)
fit <- fitdistr(x, "log-normal")
fit
x
summary(x)
fit <- fitdistr(x, "lognormal")
fit <- fitdistr(x, "t")
library(ggplot2)
library(sf)
library(rnaturalearth)
library(rnaturalearthdata)
library(dplyr)
df <- data.frame(x = war$latitude,
y = war$longitude,
t = war$days_since_start,
side_b = war$side_b)
# Load the world map
world <- ne_countries(scale = "medium", returnclass = "sf")
# Define a bounding box for Israel and the surrounding region
# Adjust the range of latitudes and longitudes as necessary
region_bbox <- st_bbox(c(
xmin = 33.0,  # Adjust this for more western longitude
xmax = 37.5,  # Adjust this for more eastern longitude
ymin = 29.5,  # Adjust this for more southern latitude
ymax = 34.5   # Adjust this for more northern latitude
), crs = st_crs(4326))  # WGS84 coordinate reference system
# Crop the world map to the bounding box
region <- st_crop(world, region_bbox)
# Convert df to an sf object
df_sf <- st_as_sf(df, coords = c("y", "x"), crs = 4326)
# Plot the data with the map
p1 <- ggplot() +
geom_sf(data = region, fill = "lightgray", color = "black") +  # Region map
geom_sf(data = df_sf, aes(color = side_b), size = 0.5, alpha = 0.7) +  # Points from df
theme_minimal() +
coord_sf(xlim = c(region_bbox$xmin, region_bbox$xmax),
ylim = c(region_bbox$ymin, region_bbox$ymax),
expand = FALSE) +
labs(title = paste0("Conflict Events in Israel"),
x = "Longitude",
y = "Latitude")
p1
library(poweRlaw)
install.packages("poweRlaw")
library(poweRlaw)
pl <- displ$new(x)
pl
pl <- displ$new(x+1)
l
pl
# Estimate the parameters using maximum likelihood
pl$setXmin(est_xmin(pl))  # Estimate the xmin (cutoff for power-law behavior)
library(poweRlaw)
pl <- displ$new(x+1)
# Estimate the parameters using maximum likelihood
pl$setXmin(est_xmin(pl))  # Estimate the xmin (cutoff for power-law behavior)
pl$setPars(est_pars(pl))  # Estimate the exponent (alpha)
# View the estimated parameters
xmin <- pl$getXmin()
alpha <- pl$getPars()
xmin
alpha
estimate_pars(pl)
plot(pl)
pl$pars
bs_p = bootstrap_p(pl, no_of_sims = 1000, threads = 2)
bs_p = bootstrap_p(pl, no_of_sims = 1000, threads = 12)
bs_p
bs_p$p
bs_p$bootstraps
war$date_start
length(war$date_start)
length(unique(war$date_start))
war$date_end
war$event_duration
table(war$event_duration)
war$intertimes
x
sum(x == 0)
sum(x == 0)/length(x)
war$days_since_start
table(war$days_since_start)
table(war$days_since_start) > 1
sum(table(war$days_since_start) > 1)
table(war$days_since_start)
table(table(war$days_since_start))
table(war$days_since_start) == 67
which(table(war$days_since_start) == 67)
war$days_since_start == 9209
war[war$days_since_start == 9209,]
war[war$days_since_start == 9209,]
ww <- war[war$days_since_start == 9209,]
unique(war$days_since_start)
u <- unique(war$days_since_start)
u[1]
war[war$days_since_start == u[1],]
war[war$days_since_start == u[2],]
war[war$days_since_start == u[3],]
war[war$days_since_start == u[129],]
lapply(u, function(u) nrow(war[war$days_since_start == u,]))
how_many_u <- lapply(u, function(u) nrow(war[war$days_since_start == u,]))
names(how_many_u) <- u
how_many_u
unlist(how_many_u)
table(war$days_since_start)
table(war$days_since_start)
war[war$days_since_start == 1706,]
